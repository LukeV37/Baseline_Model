{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed63e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa73fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54da9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3bc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1)) # Sum each row\n",
    "    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n",
    "#     r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n",
    "    \n",
    "    # D^(-1/2).A.D^(-1/2)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = mx.dot(r_mat_inv)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e762d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec661dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}/train_data.txt\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 0:3], dtype=np.float32) # Processing features into a sparse matrix\n",
    "    labels = encode_onehot(idx_features_labels[:, -2]) # one-hot encoding the labels\n",
    "    \n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, -1], dtype=np.int32) # Reading node-ids\n",
    "    \n",
    "    # Creating node ids to eliminate discrepencies in node ids in the data\n",
    "    idx_map = {j: i for i, j in enumerate(idx)} # Creating index for nodes to map it in adjacency matrix\n",
    "    \n",
    "    edges_unordered = np.genfromtxt(\"{}{}/train_edges.txt\".format(path, dataset),\n",
    "                                    dtype=np.int32) # Reading edges\n",
    "    \n",
    "\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape) # Mapping node-ids in the edge list to the index\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # CHECK OUT THE DIFFERENCES BETWEEN csr_matrix (features) and coo_matrix (adj)\n",
    "    \n",
    "    # Normalizing features\n",
    "    features = feature_normalize(features)\n",
    "    \n",
    "#     # build symmetric adjacency matrix\n",
    "#     adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "#     Normalizing the adjacency matrix after adding self loops\n",
    "    adj = adj_normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Setting training, validation, and test range\n",
    "    idx_train = range(430000)\n",
    "    idx_val = range(450000, 600000)\n",
    "    idx_test = range(700000, 900000)\n",
    "\n",
    "    # Converting all matrices into pytorch tensors\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bce67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find accuracy from two tensors\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels) # Get the index of maximum value of 1 dimension and typecast to labels datatype\n",
    "    correct = preds.eq(labels).double() # Convert into double\n",
    "    correct = correct.sum() # Sum correct predictions\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da7e8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "# Class to define a neural network layer that inherits PyTorch Module\n",
    "# Check out documentaion of the base class 'Module' at:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "class GraphConvolution(Module):\n",
    "    # Each layer requires no. of input features, no. of output features, and optional bias\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        self.in_features = in_feat\n",
    "        self.out_features = out_feat\n",
    "        \n",
    "        # Using Parameter to automatically add weights and bias to learnable parameters\n",
    "        #THIS WILL BE USEFUL ONLY WHEN WE USE Module in the model\n",
    "        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_feat))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    # Function to get uniform distribution of weights and bias values\n",
    "    # Can be removed if necessary\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    # Forward function where it actually requires the input data and operations\n",
    "    def forward(self, inp, adj):\n",
    "        # Basically we multiply A.H,W\n",
    "        support = torch.mm(inp, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        \n",
    "        # Adding bias if true\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b147458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class to define the model architecture\n",
    "class GCN(nn.Module):\n",
    "    # The model needs no. of input features, no. of hidden units,\n",
    "    # no. of classes, and optional dropout\n",
    "    \n",
    "    # NOTE: We use a simply model with one hidden layer\n",
    "        # Architecture will change for deep models\n",
    "        # Ideally, we keep only a few layers in most GNNs\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # Defining one hidden layer and one output layer\n",
    "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gcn2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    # Similar to GraphConvolution, we give required input data to the forward function\n",
    "    # And specify operations - here it is activation and dropout\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gcn1(x, adj)) # Applying non-linearity on hidden layer 1\n",
    "        # Checkout difference between nn.Dropout() and F.dropout()\n",
    "        x = F.dropout(x, self.dropout, training=self.training) \n",
    "        x = self.gcn2(x, adj)\n",
    "        return F.log_softmax(x, dim=1) # Applying lograthmic softmax on output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990494b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZZ4nu dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_468257/3650411315.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "adj, features, labels, train_ids, val_ids, test_ids = load_data('/home/lvaughan/Pileup/data/','ZZ4nu')\n",
    "# load_data('/home/abagava/Datasets/Physics/','Primary')\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "\n",
    "# Using Adam optimizer. Other optimizer can be used too\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2542c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code for GPU computing\n",
    "\n",
    "# # CHANGE THIS CODE TO SUIT YOUR VERSION OF PYTORCH. THE SYNTAX OF THIS COULD VARY SIGNIFICANTLY\n",
    "\n",
    "# # If cuda is available, movie all data to gpu\n",
    "# # And preparing for CUDA operations\n",
    "#if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     train_ids.cuda()\n",
    "#     val_ids.cuda()\n",
    "#     test_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146eb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    \n",
    "    # Evaluating the model in training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # Reseting gradient at each layer to avoid exploding gradient problem\n",
    "    output = model(features, adj)\n",
    "     \n",
    "        \n",
    "    # Optimizing with nll_loss. Other losses like cross_entropy can also be used\n",
    "    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n",
    "    acc_train = accuracy(output[train_ids], labels[train_ids])\n",
    "    # backprop and optimize model parameters\n",
    "    # Not needed to specify parameters when using Parameter\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluating validation performance separately\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
    "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
    "\n",
    "\n",
    "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
    "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b32be098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
    "    acc_test = accuracy(output[test_ids], labels[test_ids])\n",
    "    \n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32da91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.7601 acc_train: 0.5287 loss_val: 0.7143 acc_val: 0.4305 time: 0.8108s\n",
      "Iteraion-0\n",
      "Total time elapsed: 0.8164s\n",
      "\n",
      "\n",
      "Epoch: 0002 loss_train: 0.7340 acc_train: 0.5414 loss_val: 0.6960 acc_val: 0.4558 time: 0.8054s\n",
      "Iteraion-1\n",
      "Total time elapsed: 1.6281s\n",
      "\n",
      "\n",
      "Epoch: 0003 loss_train: 0.7189 acc_train: 0.5067 loss_val: 0.6865 acc_val: 0.5695 time: 0.7932s\n",
      "Iteraion-2\n",
      "Total time elapsed: 2.4277s\n",
      "\n",
      "\n",
      "Epoch: 0004 loss_train: 0.7108 acc_train: 0.4672 loss_val: 0.6840 acc_val: 0.5695 time: 0.8139s\n",
      "Iteraion-3\n",
      "Total time elapsed: 3.2474s\n",
      "\n",
      "\n",
      "Epoch: 0005 loss_train: 0.7088 acc_train: 0.4520 loss_val: 0.6853 acc_val: 0.5695 time: 0.8066s\n",
      "Iteraion-4\n",
      "Total time elapsed: 4.0603s\n",
      "\n",
      "\n",
      "Epoch: 0006 loss_train: 0.7109 acc_train: 0.4544 loss_val: 0.6876 acc_val: 0.5695 time: 0.7853s\n",
      "Iteraion-5\n",
      "Total time elapsed: 4.8520s\n",
      "\n",
      "\n",
      "Epoch: 0007 loss_train: 0.7128 acc_train: 0.4641 loss_val: 0.6890 acc_val: 0.5695 time: 0.8176s\n",
      "Iteraion-6\n",
      "Total time elapsed: 5.6751s\n",
      "\n",
      "\n",
      "Epoch: 0008 loss_train: 0.7134 acc_train: 0.4707 loss_val: 0.6890 acc_val: 0.5695 time: 0.8288s\n",
      "Iteraion-7\n",
      "Total time elapsed: 6.5100s\n",
      "\n",
      "\n",
      "Epoch: 0009 loss_train: 0.7127 acc_train: 0.4732 loss_val: 0.6878 acc_val: 0.5695 time: 0.8180s\n",
      "Iteraion-8\n",
      "Total time elapsed: 7.3350s\n",
      "\n",
      "\n",
      "Epoch: 0010 loss_train: 0.7100 acc_train: 0.4725 loss_val: 0.6862 acc_val: 0.5695 time: 0.8147s\n",
      "Iteraion-9\n",
      "Total time elapsed: 8.1559s\n",
      "\n",
      "\n",
      "Epoch: 0011 loss_train: 0.7055 acc_train: 0.4697 loss_val: 0.6847 acc_val: 0.5695 time: 0.8157s\n",
      "Iteraion-10\n",
      "Total time elapsed: 8.9779s\n",
      "\n",
      "\n",
      "Epoch: 0012 loss_train: 0.7033 acc_train: 0.4651 loss_val: 0.6837 acc_val: 0.5695 time: 0.8191s\n",
      "Iteraion-11\n",
      "Total time elapsed: 9.8035s\n",
      "\n",
      "\n",
      "Epoch: 0013 loss_train: 0.7005 acc_train: 0.4607 loss_val: 0.6834 acc_val: 0.5695 time: 0.8192s\n",
      "Iteraion-12\n",
      "Total time elapsed: 10.6292s\n",
      "\n",
      "\n",
      "Epoch: 0014 loss_train: 0.6984 acc_train: 0.4591 loss_val: 0.6838 acc_val: 0.5695 time: 0.8147s\n",
      "Iteraion-13\n",
      "Total time elapsed: 11.4512s\n",
      "\n",
      "\n",
      "Epoch: 0015 loss_train: 0.6970 acc_train: 0.4560 loss_val: 0.6846 acc_val: 0.5695 time: 0.8172s\n",
      "Iteraion-14\n",
      "Total time elapsed: 12.2750s\n",
      "\n",
      "\n",
      "Epoch: 0016 loss_train: 0.6971 acc_train: 0.4561 loss_val: 0.6855 acc_val: 0.5695 time: 0.8066s\n",
      "Iteraion-15\n",
      "Total time elapsed: 13.0892s\n",
      "\n",
      "\n",
      "Epoch: 0017 loss_train: 0.6962 acc_train: 0.4620 loss_val: 0.6863 acc_val: 0.5695 time: 0.8213s\n",
      "Iteraion-16\n",
      "Total time elapsed: 13.9170s\n",
      "\n",
      "\n",
      "Epoch: 0018 loss_train: 0.6964 acc_train: 0.4634 loss_val: 0.6868 acc_val: 0.5695 time: 0.8099s\n",
      "Iteraion-17\n",
      "Total time elapsed: 14.7342s\n",
      "\n",
      "\n",
      "Epoch: 0019 loss_train: 0.6957 acc_train: 0.4685 loss_val: 0.6869 acc_val: 0.5695 time: 0.8169s\n",
      "Iteraion-18\n",
      "Total time elapsed: 15.5576s\n",
      "\n",
      "\n",
      "Epoch: 0020 loss_train: 0.6955 acc_train: 0.4680 loss_val: 0.6865 acc_val: 0.5695 time: 0.8135s\n",
      "Iteraion-19\n",
      "Total time elapsed: 16.3784s\n",
      "\n",
      "\n",
      "Epoch: 0021 loss_train: 0.6946 acc_train: 0.4690 loss_val: 0.6858 acc_val: 0.5695 time: 0.8180s\n",
      "Iteraion-20\n",
      "Total time elapsed: 17.2032s\n",
      "\n",
      "\n",
      "Epoch: 0022 loss_train: 0.6928 acc_train: 0.4677 loss_val: 0.6849 acc_val: 0.5695 time: 0.8049s\n",
      "Iteraion-21\n",
      "Total time elapsed: 18.0141s\n",
      "\n",
      "\n",
      "Epoch: 0023 loss_train: 0.6926 acc_train: 0.4649 loss_val: 0.6841 acc_val: 0.5695 time: 0.7943s\n",
      "Iteraion-22\n",
      "Total time elapsed: 18.8151s\n",
      "\n",
      "\n",
      "Epoch: 0024 loss_train: 0.6906 acc_train: 0.4672 loss_val: 0.6833 acc_val: 0.5695 time: 0.7872s\n",
      "Iteraion-23\n",
      "Total time elapsed: 19.6076s\n",
      "\n",
      "\n",
      "Epoch: 0025 loss_train: 0.6903 acc_train: 0.4699 loss_val: 0.6828 acc_val: 0.5695 time: 0.7952s\n",
      "Iteraion-24\n",
      "Total time elapsed: 20.4080s\n",
      "\n",
      "\n",
      "Epoch: 0026 loss_train: 0.6889 acc_train: 0.4773 loss_val: 0.6824 acc_val: 0.5695 time: 0.8019s\n",
      "Iteraion-25\n",
      "Total time elapsed: 21.2166s\n",
      "\n",
      "\n",
      "Epoch: 0027 loss_train: 0.6889 acc_train: 0.4861 loss_val: 0.6823 acc_val: 0.5695 time: 0.8142s\n",
      "Iteraion-26\n",
      "Total time elapsed: 22.0363s\n",
      "\n",
      "\n",
      "Epoch: 0028 loss_train: 0.6884 acc_train: 0.4949 loss_val: 0.6823 acc_val: 0.5695 time: 0.7996s\n",
      "Iteraion-27\n",
      "Total time elapsed: 22.8435s\n",
      "\n",
      "\n",
      "Epoch: 0029 loss_train: 0.6891 acc_train: 0.5025 loss_val: 0.6824 acc_val: 0.5695 time: 0.8093s\n",
      "Iteraion-28\n",
      "Total time elapsed: 23.6603s\n",
      "\n",
      "\n",
      "Epoch: 0030 loss_train: 0.6881 acc_train: 0.5101 loss_val: 0.6824 acc_val: 0.5695 time: 0.8239s\n",
      "Iteraion-29\n",
      "Total time elapsed: 24.4909s\n",
      "\n",
      "\n",
      "Epoch: 0031 loss_train: 0.6882 acc_train: 0.5146 loss_val: 0.6823 acc_val: 0.5695 time: 0.8173s\n",
      "Iteraion-30\n",
      "Total time elapsed: 25.3149s\n",
      "\n",
      "\n",
      "Epoch: 0032 loss_train: 0.6883 acc_train: 0.5161 loss_val: 0.6822 acc_val: 0.5695 time: 0.8071s\n",
      "Iteraion-31\n",
      "Total time elapsed: 26.1283s\n",
      "\n",
      "\n",
      "Epoch: 0033 loss_train: 0.6870 acc_train: 0.5184 loss_val: 0.6821 acc_val: 0.5695 time: 0.8193s\n",
      "Iteraion-32\n",
      "Total time elapsed: 26.9552s\n",
      "\n",
      "\n",
      "Epoch: 0034 loss_train: 0.6871 acc_train: 0.5153 loss_val: 0.6820 acc_val: 0.5695 time: 0.8155s\n",
      "Iteraion-33\n",
      "Total time elapsed: 27.7783s\n",
      "\n",
      "\n",
      "Epoch: 0035 loss_train: 0.6863 acc_train: 0.5135 loss_val: 0.6820 acc_val: 0.5695 time: 0.8277s\n",
      "Iteraion-34\n",
      "Total time elapsed: 28.6128s\n",
      "\n",
      "\n",
      "Epoch: 0036 loss_train: 0.6862 acc_train: 0.5119 loss_val: 0.6821 acc_val: 0.5695 time: 0.8134s\n",
      "Iteraion-35\n",
      "Total time elapsed: 29.4337s\n",
      "\n",
      "\n",
      "Epoch: 0037 loss_train: 0.6862 acc_train: 0.5093 loss_val: 0.6821 acc_val: 0.5695 time: 0.8226s\n",
      "Iteraion-36\n",
      "Total time elapsed: 30.2627s\n",
      "\n",
      "\n",
      "Epoch: 0038 loss_train: 0.6855 acc_train: 0.5090 loss_val: 0.6822 acc_val: 0.5695 time: 0.8125s\n",
      "Iteraion-37\n",
      "Total time elapsed: 31.0825s\n",
      "\n",
      "\n",
      "Epoch: 0039 loss_train: 0.6856 acc_train: 0.5077 loss_val: 0.6822 acc_val: 0.5695 time: 0.8075s\n",
      "Iteraion-38\n",
      "Total time elapsed: 31.8967s\n",
      "\n",
      "\n",
      "Epoch: 0040 loss_train: 0.6856 acc_train: 0.5080 loss_val: 0.6821 acc_val: 0.5695 time: 0.8335s\n",
      "Iteraion-39\n",
      "Total time elapsed: 32.7368s\n",
      "\n",
      "\n",
      "Epoch: 0041 loss_train: 0.6851 acc_train: 0.5110 loss_val: 0.6819 acc_val: 0.5695 time: 0.8193s\n",
      "Iteraion-40\n",
      "Total time elapsed: 33.5612s\n",
      "\n",
      "\n",
      "Epoch: 0042 loss_train: 0.6849 acc_train: 0.5150 loss_val: 0.6817 acc_val: 0.5695 time: 0.7913s\n",
      "Iteraion-41\n",
      "Total time elapsed: 34.3584s\n",
      "\n",
      "\n",
      "Epoch: 0043 loss_train: 0.6844 acc_train: 0.5190 loss_val: 0.6815 acc_val: 0.5695 time: 0.7901s\n",
      "Iteraion-42\n",
      "Total time elapsed: 35.1555s\n",
      "\n",
      "\n",
      "Epoch: 0044 loss_train: 0.6842 acc_train: 0.5238 loss_val: 0.6812 acc_val: 0.5695 time: 0.7950s\n",
      "Iteraion-43\n",
      "Total time elapsed: 35.9558s\n",
      "\n",
      "\n",
      "Epoch: 0045 loss_train: 0.6843 acc_train: 0.5292 loss_val: 0.6810 acc_val: 0.5695 time: 0.7986s\n",
      "Iteraion-44\n",
      "Total time elapsed: 36.7605s\n",
      "\n",
      "\n",
      "Epoch: 0046 loss_train: 0.6839 acc_train: 0.5352 loss_val: 0.6808 acc_val: 0.5695 time: 0.7928s\n",
      "Iteraion-45\n",
      "Total time elapsed: 37.5586s\n",
      "\n",
      "\n",
      "Epoch: 0047 loss_train: 0.6837 acc_train: 0.5381 loss_val: 0.6806 acc_val: 0.5695 time: 0.8000s\n",
      "Iteraion-46\n",
      "Total time elapsed: 38.3661s\n",
      "\n",
      "\n",
      "Epoch: 0048 loss_train: 0.6833 acc_train: 0.5393 loss_val: 0.6805 acc_val: 0.5695 time: 0.8236s\n",
      "Iteraion-47\n",
      "Total time elapsed: 39.1959s\n",
      "\n",
      "\n",
      "Epoch: 0049 loss_train: 0.6833 acc_train: 0.5394 loss_val: 0.6804 acc_val: 0.5695 time: 0.8264s\n",
      "Iteraion-48\n",
      "Total time elapsed: 40.0292s\n",
      "\n",
      "\n",
      "Epoch: 0050 loss_train: 0.6829 acc_train: 0.5398 loss_val: 0.6802 acc_val: 0.5695 time: 0.7922s\n",
      "Iteraion-49\n",
      "Total time elapsed: 40.8274s\n",
      "\n",
      "\n",
      "Epoch: 0051 loss_train: 0.6827 acc_train: 0.5397 loss_val: 0.6801 acc_val: 0.5695 time: 0.7940s\n",
      "Iteraion-50\n",
      "Total time elapsed: 41.6318s\n",
      "\n",
      "\n",
      "Epoch: 0052 loss_train: 0.6823 acc_train: 0.5398 loss_val: 0.6800 acc_val: 0.5695 time: 0.7922s\n",
      "Iteraion-51\n",
      "Total time elapsed: 42.4302s\n",
      "\n",
      "\n",
      "Epoch: 0053 loss_train: 0.6823 acc_train: 0.5399 loss_val: 0.6798 acc_val: 0.5695 time: 0.7958s\n",
      "Iteraion-52\n",
      "Total time elapsed: 43.2313s\n",
      "\n",
      "\n",
      "Epoch: 0054 loss_train: 0.6819 acc_train: 0.5404 loss_val: 0.6796 acc_val: 0.5695 time: 0.7921s\n",
      "Iteraion-53\n",
      "Total time elapsed: 44.0296s\n",
      "\n",
      "\n",
      "Epoch: 0055 loss_train: 0.6816 acc_train: 0.5414 loss_val: 0.6794 acc_val: 0.5695 time: 0.7925s\n",
      "Iteraion-54\n",
      "Total time elapsed: 44.8274s\n",
      "\n",
      "\n",
      "Epoch: 0056 loss_train: 0.6816 acc_train: 0.5422 loss_val: 0.6792 acc_val: 0.5695 time: 0.8034s\n",
      "Iteraion-55\n",
      "Total time elapsed: 45.6360s\n",
      "\n",
      "\n",
      "Epoch: 0057 loss_train: 0.6811 acc_train: 0.5439 loss_val: 0.6789 acc_val: 0.5695 time: 0.8128s\n",
      "Iteraion-56\n",
      "Total time elapsed: 46.4558s\n",
      "\n",
      "\n",
      "Epoch: 0058 loss_train: 0.6807 acc_train: 0.5451 loss_val: 0.6786 acc_val: 0.5695 time: 0.8178s\n",
      "Iteraion-57\n",
      "Total time elapsed: 47.2807s\n",
      "\n",
      "\n",
      "Epoch: 0059 loss_train: 0.6807 acc_train: 0.5462 loss_val: 0.6783 acc_val: 0.5695 time: 0.8118s\n",
      "Iteraion-58\n",
      "Total time elapsed: 48.0999s\n",
      "\n",
      "\n",
      "Epoch: 0060 loss_train: 0.6805 acc_train: 0.5462 loss_val: 0.6780 acc_val: 0.5695 time: 0.8341s\n",
      "Iteraion-59\n",
      "Total time elapsed: 48.9415s\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0061 loss_train: 0.6799 acc_train: 0.5470 loss_val: 0.6777 acc_val: 0.5695 time: 0.8167s\n",
      "Iteraion-60\n",
      "Total time elapsed: 49.7648s\n",
      "\n",
      "\n",
      "Epoch: 0062 loss_train: 0.6796 acc_train: 0.5479 loss_val: 0.6774 acc_val: 0.5695 time: 0.8320s\n",
      "Iteraion-61\n",
      "Total time elapsed: 50.6034s\n",
      "\n",
      "\n",
      "Epoch: 0063 loss_train: 0.6789 acc_train: 0.5485 loss_val: 0.6771 acc_val: 0.5695 time: 0.8138s\n",
      "Iteraion-62\n",
      "Total time elapsed: 51.4248s\n",
      "\n",
      "\n",
      "Epoch: 0064 loss_train: 0.6791 acc_train: 0.5489 loss_val: 0.6768 acc_val: 0.5695 time: 0.8160s\n",
      "Iteraion-63\n",
      "Total time elapsed: 52.2473s\n",
      "\n",
      "\n",
      "Epoch: 0065 loss_train: 0.6784 acc_train: 0.5498 loss_val: 0.6765 acc_val: 0.5695 time: 0.8233s\n",
      "Iteraion-64\n",
      "Total time elapsed: 53.0780s\n",
      "\n",
      "\n",
      "Epoch: 0066 loss_train: 0.6784 acc_train: 0.5493 loss_val: 0.6761 acc_val: 0.5695 time: 0.8146s\n",
      "Iteraion-65\n",
      "Total time elapsed: 53.9002s\n",
      "\n",
      "\n",
      "Epoch: 0067 loss_train: 0.6780 acc_train: 0.5499 loss_val: 0.6758 acc_val: 0.5695 time: 0.8063s\n",
      "Iteraion-66\n",
      "Total time elapsed: 54.7131s\n",
      "\n",
      "\n",
      "Epoch: 0068 loss_train: 0.6778 acc_train: 0.5508 loss_val: 0.6754 acc_val: 0.5695 time: 0.8206s\n",
      "Iteraion-67\n",
      "Total time elapsed: 55.5396s\n",
      "\n",
      "\n",
      "Epoch: 0069 loss_train: 0.6772 acc_train: 0.5510 loss_val: 0.6751 acc_val: 0.5695 time: 0.8296s\n",
      "Iteraion-68\n",
      "Total time elapsed: 56.3751s\n",
      "\n",
      "\n",
      "Epoch: 0070 loss_train: 0.6770 acc_train: 0.5514 loss_val: 0.6747 acc_val: 0.5695 time: 0.8209s\n",
      "Iteraion-69\n",
      "Total time elapsed: 57.2033s\n",
      "\n",
      "\n",
      "Epoch: 0071 loss_train: 0.6765 acc_train: 0.5520 loss_val: 0.6743 acc_val: 0.5695 time: 0.8176s\n",
      "Iteraion-70\n",
      "Total time elapsed: 58.0275s\n",
      "\n",
      "\n",
      "Epoch: 0072 loss_train: 0.6761 acc_train: 0.5520 loss_val: 0.6740 acc_val: 0.5695 time: 0.8147s\n",
      "Iteraion-71\n",
      "Total time elapsed: 58.8490s\n",
      "\n",
      "\n",
      "Epoch: 0073 loss_train: 0.6758 acc_train: 0.5525 loss_val: 0.6736 acc_val: 0.5695 time: 0.8063s\n",
      "Iteraion-72\n",
      "Total time elapsed: 59.6621s\n",
      "\n",
      "\n",
      "Epoch: 0074 loss_train: 0.6753 acc_train: 0.5529 loss_val: 0.6732 acc_val: 0.5695 time: 0.8410s\n",
      "Iteraion-73\n",
      "Total time elapsed: 60.5101s\n",
      "\n",
      "\n",
      "Epoch: 0075 loss_train: 0.6750 acc_train: 0.5527 loss_val: 0.6727 acc_val: 0.5695 time: 0.8323s\n",
      "Iteraion-74\n",
      "Total time elapsed: 61.3489s\n",
      "\n",
      "\n",
      "Epoch: 0076 loss_train: 0.6745 acc_train: 0.5534 loss_val: 0.6723 acc_val: 0.5695 time: 0.8344s\n",
      "Iteraion-75\n",
      "Total time elapsed: 62.1900s\n",
      "\n",
      "\n",
      "Epoch: 0077 loss_train: 0.6739 acc_train: 0.5537 loss_val: 0.6719 acc_val: 0.5695 time: 0.8433s\n",
      "Iteraion-76\n",
      "Total time elapsed: 63.0399s\n",
      "\n",
      "\n",
      "Epoch: 0078 loss_train: 0.6739 acc_train: 0.5543 loss_val: 0.6714 acc_val: 0.5695 time: 0.8474s\n",
      "Iteraion-77\n",
      "Total time elapsed: 63.8937s\n",
      "\n",
      "\n",
      "Epoch: 0079 loss_train: 0.6733 acc_train: 0.5552 loss_val: 0.6710 acc_val: 0.5695 time: 0.8446s\n",
      "Iteraion-78\n",
      "Total time elapsed: 64.7450s\n",
      "\n",
      "\n",
      "Epoch: 0080 loss_train: 0.6728 acc_train: 0.5561 loss_val: 0.6705 acc_val: 0.5695 time: 0.8349s\n",
      "Iteraion-79\n",
      "Total time elapsed: 65.5874s\n",
      "\n",
      "\n",
      "Epoch: 0081 loss_train: 0.6727 acc_train: 0.5563 loss_val: 0.6701 acc_val: 0.5695 time: 0.8416s\n",
      "Iteraion-80\n",
      "Total time elapsed: 66.4354s\n",
      "\n",
      "\n",
      "Epoch: 0082 loss_train: 0.6718 acc_train: 0.5567 loss_val: 0.6696 acc_val: 0.5695 time: 0.8218s\n",
      "Iteraion-81\n",
      "Total time elapsed: 67.2635s\n",
      "\n",
      "\n",
      "Epoch: 0083 loss_train: 0.6714 acc_train: 0.5569 loss_val: 0.6691 acc_val: 0.5695 time: 0.8203s\n",
      "Iteraion-82\n",
      "Total time elapsed: 68.0908s\n",
      "\n",
      "\n",
      "Epoch: 0084 loss_train: 0.6709 acc_train: 0.5567 loss_val: 0.6686 acc_val: 0.5695 time: 0.8242s\n",
      "Iteraion-83\n",
      "Total time elapsed: 68.9223s\n",
      "\n",
      "\n",
      "Epoch: 0085 loss_train: 0.6706 acc_train: 0.5570 loss_val: 0.6681 acc_val: 0.5695 time: 0.8344s\n",
      "Iteraion-84\n",
      "Total time elapsed: 69.7631s\n",
      "\n",
      "\n",
      "Epoch: 0086 loss_train: 0.6703 acc_train: 0.5572 loss_val: 0.6677 acc_val: 0.5695 time: 0.8209s\n",
      "Iteraion-85\n",
      "Total time elapsed: 70.5912s\n",
      "\n",
      "\n",
      "Epoch: 0087 loss_train: 0.6697 acc_train: 0.5571 loss_val: 0.6672 acc_val: 0.5695 time: 0.8204s\n",
      "Iteraion-86\n",
      "Total time elapsed: 71.4167s\n",
      "\n",
      "\n",
      "Epoch: 0088 loss_train: 0.6693 acc_train: 0.5573 loss_val: 0.6666 acc_val: 0.5695 time: 0.8048s\n",
      "Iteraion-87\n",
      "Total time elapsed: 72.2278s\n",
      "\n",
      "\n",
      "Epoch: 0089 loss_train: 0.6688 acc_train: 0.5574 loss_val: 0.6661 acc_val: 0.5695 time: 0.8092s\n",
      "Iteraion-88\n",
      "Total time elapsed: 73.0424s\n",
      "\n",
      "\n",
      "Epoch: 0090 loss_train: 0.6683 acc_train: 0.5573 loss_val: 0.6656 acc_val: 0.5695 time: 0.7951s\n",
      "Iteraion-89\n",
      "Total time elapsed: 73.8433s\n",
      "\n",
      "\n",
      "Epoch: 0091 loss_train: 0.6678 acc_train: 0.5576 loss_val: 0.6650 acc_val: 0.5695 time: 0.8000s\n",
      "Iteraion-90\n",
      "Total time elapsed: 74.6490s\n",
      "\n",
      "\n",
      "Epoch: 0092 loss_train: 0.6673 acc_train: 0.5574 loss_val: 0.6645 acc_val: 0.5695 time: 0.8024s\n",
      "Iteraion-91\n",
      "Total time elapsed: 75.4573s\n",
      "\n",
      "\n",
      "Epoch: 0093 loss_train: 0.6667 acc_train: 0.5574 loss_val: 0.6639 acc_val: 0.5695 time: 0.8002s\n",
      "Iteraion-92\n",
      "Total time elapsed: 76.2632s\n",
      "\n",
      "\n",
      "Epoch: 0094 loss_train: 0.6665 acc_train: 0.5575 loss_val: 0.6633 acc_val: 0.5695 time: 0.7921s\n",
      "Iteraion-93\n",
      "Total time elapsed: 77.0611s\n",
      "\n",
      "\n",
      "Epoch: 0095 loss_train: 0.6660 acc_train: 0.5573 loss_val: 0.6627 acc_val: 0.5695 time: 0.8018s\n",
      "Iteraion-94\n",
      "Total time elapsed: 77.8680s\n",
      "\n",
      "\n",
      "Epoch: 0096 loss_train: 0.6653 acc_train: 0.5569 loss_val: 0.6620 acc_val: 0.5695 time: 0.8058s\n",
      "Iteraion-95\n",
      "Total time elapsed: 78.6802s\n",
      "\n",
      "\n",
      "Epoch: 0097 loss_train: 0.6646 acc_train: 0.5570 loss_val: 0.6611 acc_val: 0.5695 time: 0.8138s\n",
      "Iteraion-96\n",
      "Total time elapsed: 79.4991s\n",
      "\n",
      "\n",
      "Epoch: 0098 loss_train: 0.6639 acc_train: 0.5571 loss_val: 0.6601 acc_val: 0.5695 time: 0.7880s\n",
      "Iteraion-97\n",
      "Total time elapsed: 80.2930s\n",
      "\n",
      "\n",
      "Epoch: 0099 loss_train: 0.6627 acc_train: 0.5574 loss_val: 0.6592 acc_val: 0.5695 time: 0.8205s\n",
      "Iteraion-98\n",
      "Total time elapsed: 81.1186s\n",
      "\n",
      "\n",
      "Epoch: 0100 loss_train: 0.6623 acc_train: 0.5572 loss_val: 0.6585 acc_val: 0.5695 time: 0.7921s\n",
      "Iteraion-99\n",
      "Total time elapsed: 81.9158s\n",
      "\n",
      "\n",
      "Epoch: 0101 loss_train: 0.6614 acc_train: 0.5577 loss_val: 0.6579 acc_val: 0.5695 time: 0.8053s\n",
      "Iteraion-100\n",
      "Total time elapsed: 82.7285s\n",
      "\n",
      "\n",
      "Epoch: 0102 loss_train: 0.6609 acc_train: 0.5590 loss_val: 0.6574 acc_val: 0.5695 time: 0.7987s\n",
      "Iteraion-101\n",
      "Total time elapsed: 83.5322s\n",
      "\n",
      "\n",
      "Epoch: 0103 loss_train: 0.6606 acc_train: 0.5601 loss_val: 0.6569 acc_val: 0.5695 time: 0.7905s\n",
      "Iteraion-102\n",
      "Total time elapsed: 84.3284s\n",
      "\n",
      "\n",
      "Epoch: 0104 loss_train: 0.6600 acc_train: 0.5605 loss_val: 0.6563 acc_val: 0.5695 time: 0.8021s\n",
      "Iteraion-103\n",
      "Total time elapsed: 85.1355s\n",
      "\n",
      "\n",
      "Epoch: 0105 loss_train: 0.6598 acc_train: 0.5600 loss_val: 0.6555 acc_val: 0.5695 time: 0.8134s\n",
      "Iteraion-104\n",
      "Total time elapsed: 85.9550s\n",
      "\n",
      "\n",
      "Epoch: 0106 loss_train: 0.6590 acc_train: 0.5601 loss_val: 0.6547 acc_val: 0.5695 time: 0.8007s\n",
      "Iteraion-105\n",
      "Total time elapsed: 86.7624s\n",
      "\n",
      "\n",
      "Epoch: 0107 loss_train: 0.6582 acc_train: 0.5588 loss_val: 0.6539 acc_val: 0.5695 time: 0.8510s\n",
      "Iteraion-106\n",
      "Total time elapsed: 87.6187s\n",
      "\n",
      "\n",
      "Epoch: 0108 loss_train: 0.6577 acc_train: 0.5571 loss_val: 0.6531 acc_val: 0.5695 time: 0.7940s\n",
      "Iteraion-107\n",
      "Total time elapsed: 88.4177s\n",
      "\n",
      "\n",
      "Epoch: 0109 loss_train: 0.6567 acc_train: 0.5551 loss_val: 0.6523 acc_val: 0.5695 time: 0.7935s\n",
      "Iteraion-108\n",
      "Total time elapsed: 89.2177s\n",
      "\n",
      "\n",
      "Epoch: 0110 loss_train: 0.6564 acc_train: 0.5534 loss_val: 0.6514 acc_val: 0.5695 time: 0.8071s\n",
      "Iteraion-109\n",
      "Total time elapsed: 90.0355s\n",
      "\n",
      "\n",
      "Epoch: 0111 loss_train: 0.6559 acc_train: 0.5512 loss_val: 0.6504 acc_val: 0.5695 time: 0.8127s\n",
      "Iteraion-110\n",
      "Total time elapsed: 90.8535s\n",
      "\n",
      "\n",
      "Epoch: 0112 loss_train: 0.6545 acc_train: 0.5487 loss_val: 0.6490 acc_val: 0.5695 time: 0.7930s\n",
      "Iteraion-111\n",
      "Total time elapsed: 91.6526s\n",
      "\n",
      "\n",
      "Epoch: 0113 loss_train: 0.6535 acc_train: 0.5466 loss_val: 0.6480 acc_val: 0.5695 time: 0.8023s\n",
      "Iteraion-112\n",
      "Total time elapsed: 92.4602s\n",
      "\n",
      "\n",
      "Epoch: 0114 loss_train: 0.6524 acc_train: 0.5472 loss_val: 0.6473 acc_val: 0.5695 time: 0.7960s\n",
      "Iteraion-113\n",
      "Total time elapsed: 93.2620s\n",
      "\n",
      "\n",
      "Epoch: 0115 loss_train: 0.6518 acc_train: 0.5499 loss_val: 0.6465 acc_val: 0.5695 time: 0.8089s\n",
      "Iteraion-114\n",
      "Total time elapsed: 94.0777s\n",
      "\n",
      "\n",
      "Epoch: 0116 loss_train: 0.6513 acc_train: 0.5522 loss_val: 0.6456 acc_val: 0.5695 time: 0.8082s\n",
      "Iteraion-115\n",
      "Total time elapsed: 94.8910s\n",
      "\n",
      "\n",
      "Epoch: 0117 loss_train: 0.6506 acc_train: 0.5521 loss_val: 0.6448 acc_val: 0.5695 time: 0.7955s\n",
      "Iteraion-116\n",
      "Total time elapsed: 95.6923s\n",
      "\n",
      "\n",
      "Epoch: 0118 loss_train: 0.6501 acc_train: 0.5493 loss_val: 0.6438 acc_val: 0.5695 time: 0.7906s\n",
      "Iteraion-117\n",
      "Total time elapsed: 96.4881s\n",
      "\n",
      "\n",
      "Epoch: 0119 loss_train: 0.6489 acc_train: 0.5446 loss_val: 0.6423 acc_val: 0.5695 time: 0.8118s\n",
      "Iteraion-118\n",
      "Total time elapsed: 97.3057s\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0120 loss_train: 0.6479 acc_train: 0.5395 loss_val: 0.6412 acc_val: 0.5695 time: 0.7950s\n",
      "Iteraion-119\n",
      "Total time elapsed: 98.1064s\n",
      "\n",
      "\n",
      "Epoch: 0121 loss_train: 0.6467 acc_train: 0.5358 loss_val: 0.6400 acc_val: 0.5695 time: 0.7949s\n",
      "Iteraion-120\n",
      "Total time elapsed: 98.9071s\n",
      "\n",
      "\n",
      "Epoch: 0122 loss_train: 0.6456 acc_train: 0.5342 loss_val: 0.6386 acc_val: 0.5695 time: 0.7999s\n",
      "Iteraion-121\n",
      "Total time elapsed: 99.7125s\n",
      "\n",
      "\n",
      "Epoch: 0123 loss_train: 0.6445 acc_train: 0.5348 loss_val: 0.6375 acc_val: 0.5695 time: 0.8077s\n",
      "Iteraion-122\n",
      "Total time elapsed: 100.5278s\n",
      "\n",
      "\n",
      "Epoch: 0124 loss_train: 0.6435 acc_train: 0.5346 loss_val: 0.6365 acc_val: 0.5695 time: 0.8150s\n",
      "Iteraion-123\n",
      "Total time elapsed: 101.3487s\n",
      "\n",
      "\n",
      "Epoch: 0125 loss_train: 0.6428 acc_train: 0.5316 loss_val: 0.6349 acc_val: 0.5695 time: 0.8062s\n",
      "Iteraion-124\n",
      "Total time elapsed: 102.1626s\n",
      "\n",
      "\n",
      "Epoch: 0126 loss_train: 0.6413 acc_train: 0.5294 loss_val: 0.6338 acc_val: 0.5695 time: 0.8031s\n",
      "Iteraion-125\n",
      "Total time elapsed: 102.9718s\n",
      "\n",
      "\n",
      "Epoch: 0127 loss_train: 0.6403 acc_train: 0.5281 loss_val: 0.6324 acc_val: 0.5695 time: 0.8045s\n",
      "Iteraion-126\n",
      "Total time elapsed: 103.7839s\n",
      "\n",
      "\n",
      "Epoch: 0128 loss_train: 0.6396 acc_train: 0.5259 loss_val: 0.6308 acc_val: 0.5695 time: 0.8158s\n",
      "Iteraion-127\n",
      "Total time elapsed: 104.6049s\n",
      "\n",
      "\n",
      "Epoch: 0129 loss_train: 0.6378 acc_train: 0.5271 loss_val: 0.6296 acc_val: 0.5702 time: 0.7944s\n",
      "Iteraion-128\n",
      "Total time elapsed: 105.4043s\n",
      "\n",
      "\n",
      "Epoch: 0130 loss_train: 0.6366 acc_train: 0.5275 loss_val: 0.6281 acc_val: 0.5771 time: 0.7980s\n",
      "Iteraion-129\n",
      "Total time elapsed: 106.2098s\n",
      "\n",
      "\n",
      "Epoch: 0131 loss_train: 0.6354 acc_train: 0.5287 loss_val: 0.6263 acc_val: 0.5860 time: 0.8270s\n",
      "Iteraion-130\n",
      "Total time elapsed: 107.0436s\n",
      "\n",
      "\n",
      "Epoch: 0132 loss_train: 0.6339 acc_train: 0.5347 loss_val: 0.6244 acc_val: 0.5973 time: 0.7984s\n",
      "Iteraion-131\n",
      "Total time elapsed: 107.8473s\n",
      "\n",
      "\n",
      "Epoch: 0133 loss_train: 0.6327 acc_train: 0.5436 loss_val: 0.6227 acc_val: 0.6133 time: 0.8009s\n",
      "Iteraion-132\n",
      "Total time elapsed: 108.6533s\n",
      "\n",
      "\n",
      "Epoch: 0134 loss_train: 0.6303 acc_train: 0.5597 loss_val: 0.6211 acc_val: 0.6238 time: 0.7919s\n",
      "Iteraion-133\n",
      "Total time elapsed: 109.4504s\n",
      "\n",
      "\n",
      "Epoch: 0135 loss_train: 0.6292 acc_train: 0.5698 loss_val: 0.6196 acc_val: 0.6240 time: 0.8085s\n",
      "Iteraion-134\n",
      "Total time elapsed: 110.2654s\n",
      "\n",
      "\n",
      "Epoch: 0136 loss_train: 0.6281 acc_train: 0.5712 loss_val: 0.6181 acc_val: 0.6224 time: 0.8081s\n",
      "Iteraion-135\n",
      "Total time elapsed: 111.0784s\n",
      "\n",
      "\n",
      "Epoch: 0137 loss_train: 0.6270 acc_train: 0.5672 loss_val: 0.6164 acc_val: 0.6251 time: 0.8147s\n",
      "Iteraion-136\n",
      "Total time elapsed: 111.8998s\n",
      "\n",
      "\n",
      "Epoch: 0138 loss_train: 0.6255 acc_train: 0.5720 loss_val: 0.6148 acc_val: 0.6344 time: 0.8056s\n",
      "Iteraion-137\n",
      "Total time elapsed: 112.7113s\n",
      "\n",
      "\n",
      "Epoch: 0139 loss_train: 0.6243 acc_train: 0.5777 loss_val: 0.6133 acc_val: 0.6424 time: 0.7964s\n",
      "Iteraion-138\n",
      "Total time elapsed: 113.5124s\n",
      "\n",
      "\n",
      "Epoch: 0140 loss_train: 0.6228 acc_train: 0.5861 loss_val: 0.6118 acc_val: 0.6451 time: 0.7949s\n",
      "Iteraion-139\n",
      "Total time elapsed: 114.3127s\n",
      "\n",
      "\n",
      "Epoch: 0141 loss_train: 0.6216 acc_train: 0.5879 loss_val: 0.6099 acc_val: 0.6457 time: 0.8121s\n",
      "Iteraion-140\n",
      "Total time elapsed: 115.1325s\n",
      "\n",
      "\n",
      "Epoch: 0142 loss_train: 0.6199 acc_train: 0.5901 loss_val: 0.6081 acc_val: 0.6470 time: 0.8102s\n",
      "Iteraion-141\n",
      "Total time elapsed: 115.9479s\n",
      "\n",
      "\n",
      "Epoch: 0143 loss_train: 0.6185 acc_train: 0.5904 loss_val: 0.6064 acc_val: 0.6533 time: 0.7967s\n",
      "Iteraion-142\n",
      "Total time elapsed: 116.7505s\n",
      "\n",
      "\n",
      "Epoch: 0144 loss_train: 0.6171 acc_train: 0.5962 loss_val: 0.6048 acc_val: 0.6605 time: 0.7997s\n",
      "Iteraion-143\n",
      "Total time elapsed: 117.5554s\n",
      "\n",
      "\n",
      "Epoch: 0145 loss_train: 0.6156 acc_train: 0.6040 loss_val: 0.6031 acc_val: 0.6684 time: 0.8038s\n",
      "Iteraion-144\n",
      "Total time elapsed: 118.3642s\n",
      "\n",
      "\n",
      "Epoch: 0146 loss_train: 0.6142 acc_train: 0.6107 loss_val: 0.6014 acc_val: 0.6729 time: 0.8179s\n",
      "Iteraion-145\n",
      "Total time elapsed: 119.1871s\n",
      "\n",
      "\n",
      "Epoch: 0147 loss_train: 0.6128 acc_train: 0.6151 loss_val: 0.5997 acc_val: 0.6770 time: 0.8226s\n",
      "Iteraion-146\n",
      "Total time elapsed: 120.0143s\n",
      "\n",
      "\n",
      "Epoch: 0148 loss_train: 0.6113 acc_train: 0.6189 loss_val: 0.5980 acc_val: 0.6815 time: 0.8214s\n",
      "Iteraion-147\n",
      "Total time elapsed: 120.8410s\n",
      "\n",
      "\n",
      "Epoch: 0149 loss_train: 0.6104 acc_train: 0.6222 loss_val: 0.5964 acc_val: 0.6851 time: 0.8068s\n",
      "Iteraion-148\n",
      "Total time elapsed: 121.6528s\n",
      "\n",
      "\n",
      "Epoch: 0150 loss_train: 0.6085 acc_train: 0.6267 loss_val: 0.5947 acc_val: 0.6882 time: 0.7955s\n",
      "Iteraion-149\n",
      "Total time elapsed: 122.4542s\n",
      "\n",
      "\n",
      "Epoch: 0151 loss_train: 0.6075 acc_train: 0.6301 loss_val: 0.5929 acc_val: 0.6908 time: 0.7952s\n",
      "Iteraion-150\n",
      "Total time elapsed: 123.2551s\n",
      "\n",
      "\n",
      "Epoch: 0152 loss_train: 0.6062 acc_train: 0.6328 loss_val: 0.5912 acc_val: 0.6956 time: 0.7888s\n",
      "Iteraion-151\n",
      "Total time elapsed: 124.0489s\n",
      "\n",
      "\n",
      "Epoch: 0153 loss_train: 0.6045 acc_train: 0.6369 loss_val: 0.5895 acc_val: 0.6995 time: 0.7919s\n",
      "Iteraion-152\n",
      "Total time elapsed: 124.8521s\n",
      "\n",
      "\n",
      "Epoch: 0154 loss_train: 0.6032 acc_train: 0.6414 loss_val: 0.5879 acc_val: 0.7023 time: 0.8104s\n",
      "Iteraion-153\n",
      "Total time elapsed: 125.6674s\n",
      "\n",
      "\n",
      "Epoch: 0155 loss_train: 0.6016 acc_train: 0.6454 loss_val: 0.5862 acc_val: 0.7043 time: 0.8031s\n",
      "Iteraion-154\n",
      "Total time elapsed: 126.4762s\n",
      "\n",
      "\n",
      "Epoch: 0156 loss_train: 0.6002 acc_train: 0.6475 loss_val: 0.5845 acc_val: 0.7066 time: 0.7918s\n",
      "Iteraion-155\n",
      "Total time elapsed: 127.2729s\n",
      "\n",
      "\n",
      "Epoch: 0157 loss_train: 0.5995 acc_train: 0.6482 loss_val: 0.5830 acc_val: 0.7099 time: 0.7828s\n",
      "Iteraion-156\n",
      "Total time elapsed: 128.0614s\n",
      "\n",
      "\n",
      "Epoch: 0158 loss_train: 0.5977 acc_train: 0.6504 loss_val: 0.5814 acc_val: 0.7126 time: 0.7953s\n",
      "Iteraion-157\n",
      "Total time elapsed: 128.8627s\n",
      "\n",
      "\n",
      "Epoch: 0159 loss_train: 0.5966 acc_train: 0.6547 loss_val: 0.5798 acc_val: 0.7143 time: 0.8103s\n",
      "Iteraion-158\n",
      "Total time elapsed: 129.6778s\n",
      "\n",
      "\n",
      "Epoch: 0160 loss_train: 0.5953 acc_train: 0.6546 loss_val: 0.5782 acc_val: 0.7159 time: 0.8138s\n",
      "Iteraion-159\n",
      "Total time elapsed: 130.4963s\n",
      "\n",
      "\n",
      "Epoch: 0161 loss_train: 0.5942 acc_train: 0.6564 loss_val: 0.5767 acc_val: 0.7169 time: 0.8019s\n",
      "Iteraion-160\n",
      "Total time elapsed: 131.3036s\n",
      "\n",
      "\n",
      "Epoch: 0162 loss_train: 0.5934 acc_train: 0.6574 loss_val: 0.5752 acc_val: 0.7194 time: 0.7991s\n",
      "Iteraion-161\n",
      "Total time elapsed: 132.1086s\n",
      "\n",
      "\n",
      "Epoch: 0163 loss_train: 0.5923 acc_train: 0.6601 loss_val: 0.5738 acc_val: 0.7211 time: 0.7971s\n",
      "Iteraion-162\n",
      "Total time elapsed: 132.9115s\n",
      "\n",
      "\n",
      "Epoch: 0164 loss_train: 0.5907 acc_train: 0.6622 loss_val: 0.5723 acc_val: 0.7226 time: 0.8027s\n",
      "Iteraion-163\n",
      "Total time elapsed: 133.7202s\n",
      "\n",
      "\n",
      "Epoch: 0165 loss_train: 0.5901 acc_train: 0.6625 loss_val: 0.5708 acc_val: 0.7246 time: 0.7995s\n",
      "Iteraion-164\n",
      "Total time elapsed: 134.5247s\n",
      "\n",
      "\n",
      "Epoch: 0166 loss_train: 0.5886 acc_train: 0.6655 loss_val: 0.5695 acc_val: 0.7265 time: 0.8069s\n",
      "Iteraion-165\n",
      "Total time elapsed: 135.3366s\n",
      "\n",
      "\n",
      "Epoch: 0167 loss_train: 0.5882 acc_train: 0.6664 loss_val: 0.5680 acc_val: 0.7265 time: 0.8078s\n",
      "Iteraion-166\n",
      "Total time elapsed: 136.1496s\n",
      "\n",
      "\n",
      "Epoch: 0168 loss_train: 0.5864 acc_train: 0.6673 loss_val: 0.5666 acc_val: 0.7270 time: 0.8164s\n",
      "Iteraion-167\n",
      "Total time elapsed: 136.9725s\n",
      "\n",
      "\n",
      "Epoch: 0169 loss_train: 0.5855 acc_train: 0.6681 loss_val: 0.5653 acc_val: 0.7284 time: 0.8199s\n",
      "Iteraion-168\n",
      "Total time elapsed: 137.7990s\n",
      "\n",
      "\n",
      "Epoch: 0170 loss_train: 0.5839 acc_train: 0.6719 loss_val: 0.5640 acc_val: 0.7300 time: 0.8255s\n",
      "Iteraion-169\n",
      "Total time elapsed: 138.6307s\n",
      "\n",
      "\n",
      "Epoch: 0171 loss_train: 0.5831 acc_train: 0.6715 loss_val: 0.5627 acc_val: 0.7305 time: 0.7923s\n",
      "Iteraion-170\n",
      "Total time elapsed: 139.4289s\n",
      "\n",
      "\n",
      "Epoch: 0172 loss_train: 0.5820 acc_train: 0.6720 loss_val: 0.5614 acc_val: 0.7312 time: 0.8220s\n",
      "Iteraion-171\n",
      "Total time elapsed: 140.2560s\n",
      "\n",
      "\n",
      "Epoch: 0173 loss_train: 0.5819 acc_train: 0.6717 loss_val: 0.5603 acc_val: 0.7328 time: 0.7896s\n",
      "Iteraion-172\n",
      "Total time elapsed: 141.0516s\n",
      "\n",
      "\n",
      "Epoch: 0174 loss_train: 0.5805 acc_train: 0.6742 loss_val: 0.5591 acc_val: 0.7340 time: 0.8128s\n",
      "Iteraion-173\n",
      "Total time elapsed: 141.8709s\n",
      "\n",
      "\n",
      "Epoch: 0175 loss_train: 0.5791 acc_train: 0.6763 loss_val: 0.5578 acc_val: 0.7338 time: 0.8160s\n",
      "Iteraion-174\n",
      "Total time elapsed: 142.6919s\n",
      "\n",
      "\n",
      "Epoch: 0176 loss_train: 0.5786 acc_train: 0.6759 loss_val: 0.5566 acc_val: 0.7347 time: 0.7957s\n",
      "Iteraion-175\n",
      "Total time elapsed: 143.4932s\n",
      "\n",
      "\n",
      "Epoch: 0177 loss_train: 0.5778 acc_train: 0.6765 loss_val: 0.5556 acc_val: 0.7365 time: 0.7867s\n",
      "Iteraion-176\n",
      "Total time elapsed: 144.2876s\n",
      "\n",
      "\n",
      "Epoch: 0178 loss_train: 0.5772 acc_train: 0.6780 loss_val: 0.5545 acc_val: 0.7367 time: 0.8160s\n",
      "Iteraion-177\n",
      "Total time elapsed: 145.1086s\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0179 loss_train: 0.5764 acc_train: 0.6774 loss_val: 0.5533 acc_val: 0.7370 time: 0.7904s\n",
      "Iteraion-178\n",
      "Total time elapsed: 145.9046s\n",
      "\n",
      "\n",
      "Epoch: 0180 loss_train: 0.5749 acc_train: 0.6799 loss_val: 0.5521 acc_val: 0.7375 time: 0.8034s\n",
      "Iteraion-179\n",
      "Total time elapsed: 146.7137s\n",
      "\n",
      "\n",
      "Epoch: 0181 loss_train: 0.5745 acc_train: 0.6796 loss_val: 0.5511 acc_val: 0.7383 time: 0.7943s\n",
      "Iteraion-180\n",
      "Total time elapsed: 147.5138s\n",
      "\n",
      "\n",
      "Epoch: 0182 loss_train: 0.5731 acc_train: 0.6825 loss_val: 0.5500 acc_val: 0.7383 time: 0.7930s\n",
      "Iteraion-181\n",
      "Total time elapsed: 148.3119s\n",
      "\n",
      "\n",
      "Epoch: 0183 loss_train: 0.5722 acc_train: 0.6823 loss_val: 0.5490 acc_val: 0.7390 time: 0.8075s\n",
      "Iteraion-182\n",
      "Total time elapsed: 149.1244s\n",
      "\n",
      "\n",
      "Epoch: 0184 loss_train: 0.5719 acc_train: 0.6827 loss_val: 0.5480 acc_val: 0.7393 time: 0.8005s\n",
      "Iteraion-183\n",
      "Total time elapsed: 149.9326s\n",
      "\n",
      "\n",
      "Epoch: 0185 loss_train: 0.5714 acc_train: 0.6823 loss_val: 0.5470 acc_val: 0.7400 time: 0.8081s\n",
      "Iteraion-184\n",
      "Total time elapsed: 150.7458s\n",
      "\n",
      "\n",
      "Epoch: 0186 loss_train: 0.5702 acc_train: 0.6841 loss_val: 0.5461 acc_val: 0.7407 time: 0.8055s\n",
      "Iteraion-185\n",
      "Total time elapsed: 151.5582s\n",
      "\n",
      "\n",
      "Epoch: 0187 loss_train: 0.5691 acc_train: 0.6856 loss_val: 0.5451 acc_val: 0.7412 time: 0.8269s\n",
      "Iteraion-186\n",
      "Total time elapsed: 152.3917s\n",
      "\n",
      "\n",
      "Epoch: 0188 loss_train: 0.5683 acc_train: 0.6860 loss_val: 0.5440 acc_val: 0.7411 time: 0.8088s\n",
      "Iteraion-187\n",
      "Total time elapsed: 153.2063s\n",
      "\n",
      "\n",
      "Epoch: 0189 loss_train: 0.5684 acc_train: 0.6857 loss_val: 0.5431 acc_val: 0.7413 time: 0.8629s\n",
      "Iteraion-188\n",
      "Total time elapsed: 154.0747s\n",
      "\n",
      "\n",
      "Epoch: 0190 loss_train: 0.5670 acc_train: 0.6865 loss_val: 0.5421 acc_val: 0.7414 time: 0.8074s\n",
      "Iteraion-189\n",
      "Total time elapsed: 154.8881s\n",
      "\n",
      "\n",
      "Epoch: 0191 loss_train: 0.5664 acc_train: 0.6861 loss_val: 0.5412 acc_val: 0.7417 time: 0.8153s\n",
      "Iteraion-190\n",
      "Total time elapsed: 155.7084s\n",
      "\n",
      "\n",
      "Epoch: 0192 loss_train: 0.5653 acc_train: 0.6874 loss_val: 0.5403 acc_val: 0.7426 time: 0.8187s\n",
      "Iteraion-191\n",
      "Total time elapsed: 156.5340s\n",
      "\n",
      "\n",
      "Epoch: 0193 loss_train: 0.5651 acc_train: 0.6881 loss_val: 0.5396 acc_val: 0.7435 time: 0.8194s\n",
      "Iteraion-192\n",
      "Total time elapsed: 157.3584s\n",
      "\n",
      "\n",
      "Epoch: 0194 loss_train: 0.5649 acc_train: 0.6888 loss_val: 0.5387 acc_val: 0.7444 time: 0.8006s\n",
      "Iteraion-193\n",
      "Total time elapsed: 158.1650s\n",
      "\n",
      "\n",
      "Epoch: 0195 loss_train: 0.5639 acc_train: 0.6895 loss_val: 0.5378 acc_val: 0.7443 time: 0.7977s\n",
      "Iteraion-194\n",
      "Total time elapsed: 158.9703s\n",
      "\n",
      "\n",
      "Epoch: 0196 loss_train: 0.5634 acc_train: 0.6893 loss_val: 0.5370 acc_val: 0.7451 time: 0.8039s\n",
      "Iteraion-195\n",
      "Total time elapsed: 159.7794s\n",
      "\n",
      "\n",
      "Epoch: 0197 loss_train: 0.5631 acc_train: 0.6900 loss_val: 0.5361 acc_val: 0.7449 time: 0.8110s\n",
      "Iteraion-196\n",
      "Total time elapsed: 160.5964s\n",
      "\n",
      "\n",
      "Epoch: 0198 loss_train: 0.5615 acc_train: 0.6909 loss_val: 0.5352 acc_val: 0.7450 time: 0.8257s\n",
      "Iteraion-197\n",
      "Total time elapsed: 161.4297s\n",
      "\n",
      "\n",
      "Epoch: 0199 loss_train: 0.5617 acc_train: 0.6901 loss_val: 0.5345 acc_val: 0.7460 time: 0.8105s\n",
      "Iteraion-198\n",
      "Total time elapsed: 162.2470s\n",
      "\n",
      "\n",
      "Epoch: 0200 loss_train: 0.5611 acc_train: 0.6904 loss_val: 0.5339 acc_val: 0.7469 time: 0.8250s\n",
      "Iteraion-199\n",
      "Total time elapsed: 163.0771s\n",
      "\n",
      "\n",
      "\n",
      "Optimization finished...\n",
      "Total time elapsed: 163.0773s\n",
      "Test set results: loss= 0.5346 accuracy= 0.7477\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(400):\n",
    "    train(epoch)\n",
    "    print('Iteraion-' + str(epoch))\n",
    "    print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
    "    print('\\n')\n",
    "\n",
    "print()\n",
    "print('Optimization finished...')\n",
    "print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e6068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c661fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLLOWING SET OF CODE IS TO UNDERSTAND THE DATA\n",
    "# IT IS NOT PART OF GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46a733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample program to understand the data\n",
    "# phyData_path = '/home/abagava/Datasets/Physics/Primary/'\n",
    "# phyData_train_edges = 'train_edges.txt'\n",
    "# phyData_train_feat = 'train_data.txt'\n",
    "# phyData_test_edges = 'test_edges.txt'\n",
    "# phyData_test_feat = 'test_data.txt'\n",
    "\n",
    "# train_nodes = set()\n",
    "# test_nodes = set()\n",
    "\n",
    "# for i,line in enumerate(open(phyData_path + phyData_train_edges, 'r')):\n",
    "#     edges = line.split()\n",
    "#     train_nodes.update(edges)\n",
    "    \n",
    "# print('Train data contains ' + str(i) + ' edges.')\n",
    "# print('Validation is 30% ' + str(0.3*i) + ' edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a57f40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_at_train = set()\n",
    "\n",
    "# for i,line in enumerate(open(phyData_path + phyData_test_edges, 'r')):\n",
    "#     edges = line.split()\n",
    "    \n",
    "#     for node in edges:\n",
    "#         if node in train_nodes:\n",
    "#             nodes_at_train.add(node)\n",
    "    \n",
    "#     test_nodes.update(edges)\n",
    "    \n",
    "# print('Test data contains ' + str(i) + ' edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2141bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of training nodes: ' + str(len(train_nodes)))\n",
    "# print('Number of test nodes: ' + str(len(test_nodes)))\n",
    "# print('No. of test nodes present in training: ' + str(len(nodes_at_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b0bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = open(phyData_path + phyData_train_feat, 'r')\n",
    "\n",
    "# feat_nodes = set()\n",
    "\n",
    "# present = 0\n",
    "# not_present = 0\n",
    "\n",
    "# for i, line in enumerate(fp):\n",
    "#     feat_nodes.add(line.split()[-1])\n",
    "# fp.close()\n",
    "\n",
    "\n",
    "# # Adding 0's as features for nodes that lack features\n",
    "\n",
    "# fp = open(phyData_path + phyData_train_feat, 'a')\n",
    "# for node in train_nodes:\n",
    "#     if node not in feat_nodes:\n",
    "#         fp.write('0.0 0.0 0.0 0 ' + str(node)+' \\n')\n",
    "# #     if node in feat_nodes:\n",
    "# #         present += 1\n",
    "# #     else:\n",
    "# #         not_present += 1\n",
    "# fp.close()\n",
    "\n",
    "# print(present)\n",
    "# print(not_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d416beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = open(phyData_path + 'train_data-Copy1.txt', 'r')\n",
    "\n",
    "# for i, line in enumerate(fp):\n",
    "#     feat_nodes.add(line.split()[-1])\n",
    "# fp.close()\n",
    "\n",
    "# for node in train_nodes:\n",
    "#     if node in feat_nodes:\n",
    "#         present += 1\n",
    "#     else:\n",
    "#         not_present += 1\n",
    "    \n",
    "# print(present)\n",
    "# print(not_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af85139",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
